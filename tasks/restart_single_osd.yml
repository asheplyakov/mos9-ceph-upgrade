---

- set_fact:
    osd_id: "{{ item }}"
    restart_osd: true

- name: find out osd.{{ osd_id }} version
  command: >
    ceph daemon osd.{{ osd_id }} version
  register:
    running_osd_version

- name: find out OSD data owner
  stat: >
    path=/var/lib/ceph/osd/{{ cluster_name }}-{{ osd_id }}
    get_checksum=false
    get_md5=false
    mime=false
  register: osd_dir_stat

# note: OSD journal can be located on a different block device (partition)
- name: find out OSD journal owner
  stat: >
    path=/var/lib/ceph/osd/{{ cluster_name }}-{{ osd_id }}/journal
    follow=true
    get_checksum=false
    get_md5=false
    mime=false
  register: osd_journal_stat

- name: check if osd.{{ osd_id }} should be restarted
  set_fact:
    restart_osd: false
  when:
    - "{{ (running_osd_version.stdout|from_json).version == ceph_candidate_package_version.stdout }}"
    - "{{ ceph_user.stdout == '' or osd_dir_stat.stat.pw_name == ceph_user.stdout }}"
    - "{{ ceph_user.stdout == '' or osd_journal_stat.stat.pw_name == ceph_user.stdout }}"

- name: find out current OSD map flags, p1
  command: ceph osd getmap -o /tmp/osdmap.bin
  delegate_to: "{{ groups.mons[0] }}"
  when:
    - "{{ restart_osd|bool }}"

- name: find out current OSD map flags, part 2
  shell: >
    osdmaptool --print /tmp/osdmap.bin |
    awk '/^flags/ { gsub(",", "\",\"", $2); print "[\"" $2 "\"]" }'
  register: osdmap_flags
  delegate_to: "{{ groups.mons[0] }}"
  when:
    - "{{ restart_osd|bool }}"

- name: disable data rebalancing and scrubbing
  command: >
    ceph osd set {{ item }}
  with_items:
    - noout
    - noscrub
    - nodeep-scrub
  when:
    - "{{ restart_osd|bool }}"
    - "{{ item not in (osdmap_flags.stdout|from_json) }}"
  delegate_to: "{{ groups.mons[0] }}"

- name: stop OSD {{ osd_id }} (upstart)
  service: >
    name=ceph-osd
    state=stopped
    args=id={{ osd_id }}
  when:
    - "{{ restart_osd|bool }}"
    - ansible_service_mgr == "upstart"

- name: stop OSD {{ osd_id }} (systemd)
  service: >
    name=ceph-osd@{{ osd_id }}
    state=stopped
  when:
    - "{{ restart_osd|bool }}"
    - ansible_service_mgr == "systemd"

- name: fix OSD data ownership
  file: >
    path=/var/lib/ceph/osd/{{ cluster_name }}-{{ osd_id }}
    state=directory
    owner={{ ceph_user.stdout }}
    group={{ ceph_group.stdout }}
    mode=0770
    recurse=yes
  when:
    - ceph_user.stdout != ''
    - osd_dir_stat.stat.pw_name != ceph_user.stdout
    - "{{ restart_osd|bool }}"

- name: fix OSD journal ownership
  file: >
    path=/var/lib/ceph/osd/{{ cluster_name }}-{{ osd_id }}/journal
    follow=yes
    state=file
    owner={{ ceph_user.stdout }}
    group={{ ceph_group.stdout }}
    mode=0770
  when:
    - ceph_user.stdout != ''
    - osd_journal_stat.stat.pw_name != ceph_user.stdout
    - "{{ restart_osd|bool }}"

- name: start OSD {{ osd_id }} (upstart)
  service: >
    name=ceph-osd
    state=started
    args=id={{ osd_id }}
  when:
    - "{{ restart_osd|bool }}"
    - ansible_service_mgr == "upstart"

- name: start OSD {{ osd_id }} (systemd)
  service: >
    name=ceph-osd@{{ osd_id }}
    state=started
  when:
    - "{{ restart_osd|bool }}"
    - ansible_service_mgr == "systemd"

# Wait until all PGs are in the `active+clean' state.

- name: wait for PGs to become active+clean
  command: ceph -s --format=json
  register: ceph_health_post
  delegate_to: "{{ groups.mons[0] }}"
  until: (ceph_health_post.stdout|from_json).pgmap.pgs_by_state|length == 1 and
         (ceph_health_post.stdout|from_json).pgmap.pgs_by_state.0.state_name == "active+clean"
  retries: "{{ osd_restart_attempts }}"
  delay: "{{ osd_restart_delay }}"
  when: "{{ restart_osd|bool }}"
